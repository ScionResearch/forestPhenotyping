{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import the different libraries\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## Import the GBM libraries\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor, DMatrix\n",
    "import catboost as cb\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import lightgbm as lgb\n",
    "\n",
    "## Import the validation metrics\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "\n",
    "from paramsearch import paramsearch\n",
    "from itertools import product,chain\n",
    "\n",
    "import pickle\n",
    "\n",
    "## Import the plotting libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size']=7\n",
    "plt.rcParams['savefig.dpi']=750\n",
    "\n",
    "import time\n",
    "\n",
    "import numbers\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "warnings.filterwarnings(\"ignore\", message=\"numpy.dtype size changed\")\n",
    "\n",
    "## Import the shapley library\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "## Initialise path \n",
    "tempTemporaryPath= \"./saveVariables/\"\n",
    "if not os.path.exists(tempTemporaryPath):\n",
    "    os.makedirs(tempTemporaryPath)\n",
    "    \n",
    "methodCase=['xgb', 'cat', 'lgb']\n",
    "for item in methodCase:\n",
    "    if not os.path.exists(tempTemporaryPath + item + '_save'):\n",
    "        os.makedirs(tempTemporaryPath + item + '_save')\n",
    "    \n",
    "    \n",
    "    \n",
    "## load data from parquet\n",
    "# df_enc= pd.read_parquet('../data/preprocessEncData.parquet')\n",
    "## load data from parquet\n",
    "df_cat= pd.read_parquet('../data/preprocessWCatData.parquet')\n",
    "\n",
    "for item in ['PruneDate', 'ThinDate', 'soil_final']:\n",
    "    df_cat.loc[df_cat[item]<0, item]= np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Choose depend variable\n",
    "predictor= \"SiteIndex\"\n",
    "## Remove some controversial features\n",
    "list2Drop = [\"Index\",\"Age\",\"Latitude\", \"Longitude\", \"SiteIndex\" if predictor==\"I300\" else \"I300\"]\n",
    "# df_enc= df_enc.drop(list2Drop, axis=1)\n",
    "df_cat= df_cat.drop(list2Drop, axis=1)\n",
    "## Inform about the names of the categorical features\n",
    "catFeatures=['ThinType', 'Clone', 'ThinClass', 'PruneClass', 'Seedlot.Planting.Stock', \n",
    "             'Seedlot.Planting.Stock.Type', 'SeedlotCod', 'soil_final']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Remove the NaN from categorical features\n",
    "df_cat.update(df_cat[catFeatures].fillna(-1000))\n",
    "df_cat[catFeatures]=df_cat[catFeatures].astype(int)\n",
    "for item in catFeatures:\n",
    "    df_cat[item]=df_cat[item].astype(\"category\").cat.codes + 1\n",
    "    \n",
    "## Do a copy, for later estimations, and remove the missing dependant variable observations\n",
    "df_cat_copy= df_cat\n",
    "df_cat = df_cat[pd.notnull(df_cat[predictor])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into X and y\n",
    "X_cat = df_cat.iloc[:,1:]\n",
    "Y = df_cat[[predictor]]\n",
    "\n",
    "cat_features_index= [X_cat.columns.get_loc(c) for c in catFeatures if c in X_cat]\n",
    "\n",
    "# split categorical data into train and test sets\n",
    "seedTest = 6969\n",
    "test_size = 0.30\n",
    "Xcat_train, Xcat_test, Ycat_train, Ycat_test = train_test_split(X_cat, Y, test_size=test_size, random_state=seedTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in catFeatures:\n",
    "    print(item)\n",
    "    print(len(df_cat[item].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"saveModels/datasetCategoricalSI.dat\", \"wb\") as f:\n",
    "    pickle.dump(df_cat, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################################################\n",
    "###################################################################################\n",
    "#############################     END DATA PREPARATION     ###############################\n",
    "###################################################################################\n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################################################\n",
    "###################################################################################\n",
    "##########################     GRIDSEARCH PARAMETERS TUNING     #########################\n",
    "###################################################################################\n",
    "###################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Matrices for XGBoost\n",
    "all_matrice = xgb.DMatrix(data=X_cat, label=Y)\n",
    "train_matrice = xgb.DMatrix(Xcat_train, label = Ycat_train)\n",
    "test_matrice = xgb.DMatrix(Xcat_test, label = Ycat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_XGB = {\"max_depth\": [3, 4, 5], \n",
    "#              \"min_child_weight\" : [4, 5, 6],\n",
    "#              \"num_round\": [2000, 5000, 10000],\n",
    "#              \"learning_rate\": [0.01, 0.2, 0.9],\n",
    "#              \"gamma\": [0.0, 0.2, 0.4],\n",
    "#              'tree_method': 'gpu_hist'\n",
    "#             }\n",
    "\n",
    "param_XGB = {\"max_depth\": 4, \n",
    "             \"min_child_weight\" : 3,\n",
    "             \"num_round\": 7000,\n",
    "             \"learning_rate\": [0.05, 0.1],\n",
    "             \"gamma\": 1.0,\n",
    "             'tree_method': 'gpu_hist'\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xgboost_param_tune(params, all_matrix, n_splits=3):\n",
    "    ps = paramsearch(params)\n",
    "    for prms in chain(#ps.grid_search(['max_depth', 'min_child_weight']),\n",
    "                      #ps.grid_search([\"gamma\"])\n",
    "                      #ps.grid_search(['num_round'])\n",
    "                      ps.grid_search(['learning_rate'])\n",
    "                      ):\n",
    "        \n",
    "            print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "            print(\"%%%%%%%% NEW TRAINING ROUND %%%%%%%%%%\")\n",
    "            print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")      \n",
    "        \n",
    "#         paramDone=[items for items in ps.results]\n",
    "#         if prms in paramDone[1]:\n",
    "#             print(\"params: \" + str(prms) + \" results: \" + str(paramDone[0]))\n",
    "#         else:\n",
    "#             print(\"params: \" + str(prms))\n",
    "\n",
    "            startTime= time.time()\n",
    "            print(\"params: \" + str(prms))\n",
    "            cv_data = xgb.cv(\n",
    "                params= prms,\n",
    "                dtrain=all_matrix,\n",
    "                nfold= n_splits,\n",
    "                num_boost_round=prms[\"num_round\"],\n",
    "                seed= 42,\n",
    "                early_stopping_rounds=100\n",
    "            )\n",
    "            with open('./saveVariables/paramsXGB.pickle', 'wb') as handle:\n",
    "                pickle.dump(prms, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open('./saveVariables/cvXGB.pickle', 'wb') as handle:\n",
    "                pickle.dump(cv_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "            min_indx=cv_data.index[cv_data['test-rmse-mean']-cv_data['train-rmse-mean']>0.1]\n",
    "            \n",
    "            if len(min_indx)==0:\n",
    "                res= cv_data['test-rmse-mean'].min()\n",
    "                trainRMSE= cv_data.loc[cv_data['test-rmse-mean'].idxmin(), 'train-rmse-mean']\n",
    "            else:\n",
    "                res=cv_data.loc[min_indx[0], 'test-rmse-mean']\n",
    "                trainRMSE= cv_data.loc[min_indx[0], 'train-rmse-mean']\n",
    "                prms.update({\"num_round\": min_indx[0]})\n",
    "                print(min_indx[0])\n",
    "\n",
    "            print(time.time()- startTime)\n",
    "            print(\"res: \" + str(res))\n",
    "            \n",
    "            print(\"train: \" + str(trainRMSE))\n",
    "                  \n",
    "            \n",
    "            # save the crossvalidation result so that future iterations can reuse the best parameters\n",
    "            ps.register_result(-res,prms)\n",
    "\n",
    "            with open('./saveVariables/psXGB.pickle', 'wb') as handle:\n",
    "                pickle.dump(ps, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    return(ps.bestparam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "startTime=time.time()\n",
    "bestparams = xgboost_param_tune(param_XGB, train_matrice)\n",
    "print(time.time()- startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestparams_XGB=bestparams\n",
    "bestparams_XGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Training XGB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime=time.time()\n",
    "modelXGB = xgb.train(bestparams_XGB, train_matrice, num_boost_round=bestparams_XGB[\"num_round\"])\n",
    "print(time.time()- startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "modelXGB.dump_model('xgboost_raw.txt')\n",
    "with open(\"saveModels/xgbModel_training.dat\", \"wb\") as f:\n",
    "    pickle.dump(modelXGB, f)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load model\n",
    "with open(\"saveModels/xgbModel_training.dat\", \"rb\") as f:\n",
    "    modelXGB= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the model\n",
    "startTime=time.time()\n",
    "predictionXGB= modelXGB.predict(test_matrice)\n",
    "print(time.time()- startTime)\n",
    "print(\"R2: \" + str(r2_score(predictionXGB, Ycat_test)))\n",
    "print(\"RMSE: \" + str(np.sqrt(mean_squared_error(predictionXGB, Ycat_test))))\n",
    "\n",
    "predictionXGB2= modelXGB.predict(train_matrice)\n",
    "print(\"R2: \" + str(r2_score(predictionXGB2, Ycat_train)))\n",
    "print(\"RMSE: \" + str(np.sqrt(mean_squared_error(predictionXGB2, Ycat_train))))\n",
    "\n",
    "predictionXGB3= modelXGB.predict(all_matrice)\n",
    "print(\"R2: \" + str(r2_score(predictionXGB3, Y)))\n",
    "print(\"RMSE: \" + str(np.sqrt(mean_squared_error(predictionXGB3, Y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train final XGB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We remove the gpu for the final training, the Sharpley method crashes on gpu_hist\n",
    "bestparams_XGB['tree_method']= 'hist'\n",
    "\n",
    "startTime=time.time()\n",
    "modelXGB_final = xgb.train(bestparams_XGB, all_matrice, num_boost_round=bestparams_XGB[\"num_round\"])\n",
    "print(time.time()- startTime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"saveModels/xgboostModel.dat\", \"wb\") as f:\n",
    "    pickle.dump(modelXGB_final, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################################\n",
    "###########################################################\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Catboost Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Pools for Catboost\n",
    "all_pool= Pool(data=X_cat, label=Y, cat_features= cat_features_index, has_header=True)\n",
    "train_pool = Pool(data=Xcat_train, label=Ycat_train, cat_features= cat_features_index, has_header=True)\n",
    "test_pool = Pool(data=Xcat_test, label=Ycat_test, cat_features= cat_features_index, has_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_CAT = {'depth': 5,\n",
    "              'iterations': 12000,\n",
    "              'learning_rate':0.1,\n",
    "              'l2_leaf_reg': 5,\n",
    "              'one_hot_max_size': 10,\n",
    "              'task_type':'GPU'\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catboost_param_tune(params, all_pool, n_splits=3):\n",
    "    ps = paramsearch(params)\n",
    "    for prms in chain(#ps.grid_search(['one_hot_max_size'])\n",
    "                      #ps.grid_search(['depth'])\n",
    "                      ps.grid_search(['l2_leaf_reg'])\n",
    "                      #ps.grid_search(['learning_rate'])\n",
    "                      ):\n",
    "        \n",
    "#         paramDone=[items for items in ps.results]\n",
    "#         if prms in paramDone[1]:\n",
    "#             print(\"params: \" + str(prms) + \" results: \" + str(paramDone[0]))\n",
    "#         else:\n",
    "#             print(\"params: \" + str(prms))\n",
    "\n",
    "            print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\")\n",
    "            print(\"%%%%%%%% NEW TRAINING ROUND %%%%%%%%%%\")\n",
    "            print(\"%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\") \n",
    "\n",
    "            print(\"params: \" + str(prms))\n",
    "    \n",
    "            startTime=time.time()\n",
    "            cv_data = cb.cv(\n",
    "                all_pool,\n",
    "                prms,\n",
    "                fold_count= n_splits,\n",
    "                partition_random_seed= 42,\n",
    "                early_stopping_rounds=100,\n",
    "                plot=False,\n",
    "                verbose=False\n",
    "            )\n",
    "            with open('./saveVariables/paramsCAT.pickle', 'wb') as handle:\n",
    "                pickle.dump(prms, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open('./saveVariables/cvCAT.pickle', 'wb') as handle:\n",
    "                pickle.dump(cv_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "           \n",
    "\n",
    "            \n",
    "            min_indx=cv_data.index[cv_data['test-RMSE-mean']-cv_data['train-RMSE-mean']>0.1]\n",
    "            \n",
    "            if len(min_indx)==0:\n",
    "                res= cv_data['test-RMSE-mean'].min()\n",
    "                trainRMSE= cv_data.loc[cv_data['test-RMSE-mean'].idxmin(), 'train-RMSE-mean']\n",
    "            else:\n",
    "                res=cv_data.loc[min_indx[0], 'test-RMSE-mean']\n",
    "                trainRMSE= cv_data.loc[min_indx[0], 'train-RMSE-mean']\n",
    "                prms.update({\"iterations\": min_indx[0]})\n",
    "                print(min_indx[0])\n",
    "            \n",
    "            print(time.time()- startTime)\n",
    "            print(\"res: \" + str(res))\n",
    "            print(\"train: \" + str(trainRMSE))\n",
    "                \n",
    "            # save the crossvalidation result so that future iterations can reuse the best parameters\n",
    "            ps.register_result(-res,prms)\n",
    "\n",
    "            with open('./saveVariables/psCAT.pickle', 'wb') as handle:\n",
    "                pickle.dump(ps, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return ps.bestparam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTime=time.time()\n",
    "bestparams = catboost_param_tune(params_CAT, train_pool)\n",
    "print(time.time()- startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bestparams_CAT = {'depth': 5,\n",
    "#               'iterations': 19829,\n",
    "#               'learning_rate':0.1,\n",
    "#               'l2_leaf_reg': 50,\n",
    "#               'one_hot_max_size': 10,\n",
    "#               'task_type':'GPU'\n",
    "#              }\n",
    "\n",
    "bestparams_CAT = bestparams\n",
    "bestparams_CAT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Training CAT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the catboost model\n",
    "startTime=time.time()\n",
    "modelCAT = cb.train(train_pool, bestparams_CAT, verbose=False)\n",
    "print(time.time()-startTime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "with open(\"saveModels/catModel_training.dat\", \"wb\") as f:\n",
    "    pickle.dump(modelCAT, f)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load model\n",
    "with open(\"saveModels/catModel_training.dat\", \"rb\") as f:\n",
    "    modelCAT= pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Validate the model\n",
    "\n",
    "startTime=time.time()\n",
    "predictionCAT= modelCAT.predict(test_pool)\n",
    "print(time.time()-startTime)\n",
    "print(\"R2_test: \" + str(r2_score(predictionCAT, Ycat_test)))\n",
    "print(\"RMSE_test: \" + str(np.sqrt(mean_squared_error(predictionCAT, Ycat_test))))\n",
    "\n",
    "print(\"R2_train: \" + str(r2_score(modelCAT.predict(train_pool), Ycat_train)))\n",
    "print(\"RMSE_train: \" + str(np.sqrt(mean_squared_error(modelCAT.predict(train_pool), Ycat_train))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train final CAT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the final model for prediction\n",
    "startTime=time.time()\n",
    "modelCAT_final = cb.train(all_pool, bestparams_CAT, verbose=False)\n",
    "print(time.time()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "with open(\"saveModels/catboostModel.dat\", \"wb\") as f:\n",
    "    pickle.dump(modelCAT_final, f)\n",
    "    \n",
    "## Load model\n",
    "# with open(\"saveModels/catboostModel.dat\", \"rb\") as f:\n",
    "#     modelCAT_final= pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###########################################################\n",
    "###########################################################\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LightGBM Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catFeatures=['ThinType', 'Clone', 'ThinClass', 'PruneClass', 'Seedlot.Planting.Stock', \n",
    "             'Seedlot.Planting.Stock.Type', 'soil_final']\n",
    "cat_features_index= [X_cat.columns.get_loc(c) for c in catFeatures if c in X_cat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dataset for LightGBM\n",
    "all_dataset= lgb.Dataset(data=X_cat, label=Y, categorical_feature= cat_features_index, free_raw_data=False)\n",
    "\n",
    "# train_dataset = lgb.Dataset(data=Xcat_train, label=Ycat_train, categorical_feature= cat_features_index, free_raw_data=False)\n",
    "\n",
    "train_dataset = lgb.Dataset(data=Xcat_train, label=Ycat_train)\n",
    "test_dataset = lgb.Dataset(data=Xcat_test, label=Ycat_test, categorical_feature= cat_features_index, free_raw_data=False)\n",
    "\n",
    "# train_partial_dataset = lgb.Dataset(data=Xcat_train_partial, label=Ycat_train_partial, categorical_feature= cat_features_index, free_raw_data=False)\n",
    "# valid_dataset = lgb.Dataset(data=Xcat_valid, label=Ycat_valid, categorical_feature= cat_features_index, free_raw_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params_LGB = {'max_depth':[4, 5, 6],\n",
    "#               'n_estimators':15000,\n",
    "#               'learning_rate':[0.05, 0.1, 0.15],\n",
    "#               'num_leaves':[10, 12, 14],\n",
    "#               'objective': 'regression',\n",
    "#               'metric': 'rmse',\n",
    "#               'device':'gpu',\n",
    "#               'gpu_platform_id' :0,\n",
    "#               'gpu_device_id':0 \n",
    "#              }\n",
    "\n",
    "params_LGB = {'max_depth':4,\n",
    "              'n_estimators':25000,\n",
    "              'learning_rate':0.1,\n",
    "              'num_leaves':10,\n",
    "              'min_data_in_leaf': [100, 50, 40, 30, 20, 15, 10, 5, 2],\n",
    "              'objective': 'regression',\n",
    "              'metric': 'rmse',\n",
    "              'is_training_metric': True,\n",
    "              'device':'gpu',\n",
    "              'gpu_platform_id' :0,\n",
    "              'gpu_device_id':0 \n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lightgbm_param_tune(params, X, Y, cat_features_index, n_splits=3):\n",
    "    ps = paramsearch(params)\n",
    "    for prms in chain(ps.grid_search(['min_data_in_leaf'])\n",
    "                      #ps.grid_search(['num_leaves'])\n",
    "                      #ps.grid_search(['learning_rate'])\n",
    "                      ):\n",
    "#         paramDone=[items for items in ps.results]\n",
    "#         if prms in paramDone[1]:\n",
    "#             print(\"params: \" + str(prms) + \" results: \" + str(paramDone[0]))\n",
    "#         else:\n",
    "#             print(\"params: \" + str(prms))\n",
    "            print(\"params: \" + str(prms))\n",
    "    \n",
    "            startTime=time.time()\n",
    "            \n",
    "            \n",
    "            setValidName= ['test-RMSE', 'train-RMSE']\n",
    "\n",
    "            cv_data= pd.DataFrame()\n",
    "            kf= KFold(n_splits, random_state=42, shuffle= True)\n",
    "            for train_index, test_index in kf.split(X):\n",
    "                Xlocal_train, Xlocal_valid= X.iloc[train_index], X.iloc[test_index]\n",
    "                Ylocal_train, Ylocal_valid= Y.iloc[train_index], Y.iloc[test_index]\n",
    "\n",
    "                train_dataset = lgb.Dataset(data=Xlocal_train, label=Ylocal_train)\n",
    "                valid_dataset = lgb.Dataset(data=Xlocal_valid, label=Ylocal_valid)\n",
    "                setValid= [valid_dataset, train_dataset]\n",
    "                evalRes={}\n",
    "\n",
    "                cvLGB= lgb.train(prms, train_dataset, valid_sets= setValid, valid_names= setValidName,\n",
    "                                 early_stopping_rounds=100, verbose_eval=False, evals_result=evalRes,\n",
    "                                categorical_feature= cat_features_index)\n",
    "\n",
    "                dfRes= pd.DataFrame.from_dict(evalRes)\n",
    "                dfRes= dfRes.transpose()\n",
    "                dfRes= dfRes.rmse.apply(pd.Series)\n",
    "                dfRes= dfRes.transpose()\n",
    "                cv_data= pd.concat([cv_data, dfRes], axis=1)\n",
    "\n",
    "            cv_data=cv_data.groupby(by=cv_data.columns, axis=1).apply(lambda g: g.mean(axis=1) if isinstance(g.iloc[0,0], numbers.Number) else g.iloc[:,0])\n",
    "\n",
    "            min_indx=cv_data.index[cv_data['test-RMSE']-cv_data['train-RMSE']>0.1]\n",
    "\n",
    "\n",
    "\n",
    "            with open('./saveVariables/paramsLGB.pickle', 'wb') as handle:\n",
    "                pickle.dump(prms, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            with open('./saveVariables/cvLGB.pickle', 'wb') as handle:\n",
    "                pickle.dump(cv_data, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "               \n",
    "            print('Actual')\n",
    "            print(cv_data['test-RMSE'].min())\n",
    "            print(cv_data.loc[cv_data['test-RMSE'].idxmin(), 'train-RMSE'])\n",
    "            \n",
    "            if len(min_indx)==0:\n",
    "                res= cv_data['test-RMSE'].min()\n",
    "                trainRMSE= cv_data.loc[cv_data['test-RMSE'].idxmin(), 'train-RMSE']\n",
    "            else:\n",
    "                res=cv_data.loc[min_indx[0], 'test-RMSE']\n",
    "                trainRMSE= cv_data.loc[min_indx[0], 'train-RMSE']\n",
    "                prms.update({\"n_estimators\": min_indx[0]})\n",
    "                print(min_indx[0])\n",
    "\n",
    "                \n",
    "            print(time.time()- startTime)\n",
    "            print(\"res: \" + str(res))\n",
    "            \n",
    "            print(\"train: \" + str(trainRMSE))\n",
    "            print('---------Next---------')\n",
    "            # save the crossvalidation result so that future iterations can reuse the best parameters\n",
    "            ps.register_result(-res,prms)\n",
    "\n",
    "            with open('./saveVariables/psLGB.pickle', 'wb') as handle:\n",
    "                pickle.dump(ps, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    return ps.bestparam()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "startTime=time.time()\n",
    "best_params= lightgbm_param_tune(params_LGB, Xcat_train, Ycat_train, cat_features_index=cat_features_index)\n",
    "print(time.time()-startTime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestparams_LGB=best_params\n",
    "bestparams_LGB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Training LGB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_evalR2(y_true, y_pred):\n",
    "    return(\"r2\", [np.sqrt(mean_squared_error(y_true, y_pred.get_label())), r2_score(y_true, y_pred.get_label())], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Measure R2 during training\n",
    "startTime=time.time()\n",
    "evalFinalRes={}\n",
    "modelLGB = lgb.train(\n",
    "    params=bestparams_LGB, \n",
    "    train_set= train_dataset,\n",
    "    valid_sets= train_dataset,\n",
    "    feval= custom_evalR2,\n",
    "    verbose_eval= False, \n",
    "    evals_result=evalFinalRes,\n",
    "    categorical_feature= cat_features_index\n",
    ")\n",
    "print(time.time()-startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmseRes= pd.DataFrame.from_dict(evalFinalRes)\n",
    "metricRes= rmseRes.transpose()\n",
    "rmseRes= metricRes.rmse.apply(pd.Series)\n",
    "r2Res= metricRes.r2.apply(pd.Series)\n",
    "rmseRes= rmseRes.transpose()\n",
    "r2Res=r2Res.transpose()\n",
    "print(\"RMSE_train: \" + str(rmseRes.iloc[len(rmseRes)-1]) + \"\\n R2_train: \" + str(r2Res.iloc[len(r2Res)-1])) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLGB.save_model(\"saveModels/lightgbmModel.cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the moel\n",
    "startTime=time.time()\n",
    "# Get predictions\n",
    "predsLGB = modelLGB.predict(Xcat_test)\n",
    "print(time.time()-startTime)\n",
    "\n",
    "print(\"R2: \" + str(r2_score(Ycat_test, predsLGB)))\n",
    "print(\"RMSE: \" + str(np.sqrt(mean_squared_error(predsLGB, Ycat_test))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsLGB2 = modelLGB.predict(X_cat)\n",
    "predsLGB3 = modelLGB.predict(Xcat_train)\n",
    "\n",
    "print(\"R2: \" + str(r2_score(Y, predsLGB2)))\n",
    "print(\"RMSE: \" + str(np.sqrt(mean_squared_error(predsLGB2, Y))))\n",
    "\n",
    "print(\"R2: \" + str(r2_score(predsLGB3, Ycat_train)))\n",
    "print(\"RMSE: \" + str(np.sqrt(mean_squared_error(predsLGB3, Ycat_train))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train final LGB model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train final model for prediction\n",
    "startTime=time.time()\n",
    "modelLGB_final = lgb.train(\n",
    "    params=bestparams_LGB, \n",
    "    train_set= all_dataset,\n",
    "    verbose_eval= False, \n",
    "    categorical_feature= cat_features_index\n",
    ")\n",
    "print(time.time()-startTime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Save model\n",
    "with open(\"saveModels/lightgbmModel.dat\", \"wb\") as f:\n",
    "    pickle.dump(modelLGB_final, f)\n",
    "\n",
    "## Load model\n",
    "# with open(\"saveModels/lightgbmModel.dat\", \"rb\") as f:\n",
    "#     modelLGB_final= pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###################################################################################\n",
    "###################################################################################\n",
    "##########################     END PARAMETERS TUNING     ##########################\n",
    "###################################################################################\n",
    "###################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
